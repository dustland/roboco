{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"AgentX <p>     An open-source framework for building, observing, and orchestratingautonomous multi-agent systems.   </p>"},{"location":"#get-started-in-minutes","title":"Get Started in Minutes","text":"<p>Dive right in and build your first multi-agent application.</p>   - :rocket:{ .lg .middle } **Quickstart Guide**    ***    Follow our step-by-step guide to run a single agent and then a collaborative team. The fastest way to see AgentX in action.    [:octicons-arrow-right-24: Go to the Quickstart](quickstart.md)  - :compass:{ .lg .middle } **Architecture Deep Dive**    ***    Understand the core concepts behind AgentX, from the `TaskExecutor` to secure tool handling and natural language handoffs.    [:octicons-arrow-right-24: Read the Design Docs](docs/01-architecture.md)  - :code-square:{ .lg .middle } **API Reference**    ***    Explore the complete public API, with detailed information on the functions and classes available for building your agents.    [:octicons-arrow-right-24: Browse the API](api/index.md)"},{"location":"#our-philosophy","title":"Our Philosophy","text":"<p>AgentX is built on a few core principles:</p> <ul> <li>Autonomy: Agents should be able to work towards a goal without constant human intervention.</li> <li>Orchestration, not just Conversation: We focus on the flow of work and tasks between agents, not just the exchange of messages.</li> <li>Security by Default: Untrusted code or commands generated by AI should run in a secure, sandboxed environment.</li> <li>Developer Experience: Building complex agent systems should be as simple and intuitive as possible, with clear configuration and a powerful API.</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This guide will walk you through running your first AgentX applications, from a simple, single-agent chat to a more complex multi-agent team.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have AgentX installed and have configured your LLM API keys as environment variables (e.g., <code>DEEPSEEK_API_KEY</code>).</p>"},{"location":"quickstart/#example-1-simple-chat-single-agent","title":"Example 1: Simple Chat (Single Agent)","text":"<p>This example demonstrates the simplest use case: a direct conversation with a single, tool-equipped AI assistant. It uses the <code>start_task</code> and <code>task.step()</code> functions for an interactive, turn-by-turn conversation.</p>"},{"location":"quickstart/#1-the-code-examplessimple_chatmainpy","title":"1. The Code (<code>examples/simple_chat/main.py</code>)","text":"<p>This Python script sets up an interactive chat loop.</p> <pre><code>#!/usr/bin/env python3\nimport asyncio\nimport sys\nfrom pathlib import Path\n\nsys.path.insert(0, str(Path(__file__).parent.parent.parent / \"src\"))\n\nfrom agentx import start_task\n\nasync def main():\n    print(\"\ud83e\udd16 AgentX Chat (type 'quit' to exit)\\\\n\")\n    task = start_task(\"hi\")\n    user_input = None\n\n    while not task.is_complete:\n        print(\"\ud83e\udd16 Assistant: \", end=\"\", flush=True)\n\n        async for chunk in task.step(user_input=user_input, stream=True):\n            if chunk.get(\"type\") == \"content\":\n                print(chunk.get(\"content\", \"\"), end=\"\", flush=True)\n\n        if not task.is_complete:\n            user_input = input(\"\ud83d\udc64 You: \").strip()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"quickstart/#2-the-configuration-examplessimple_chatteamyaml","title":"2. The Configuration (<code>examples/simple_chat/team.yaml</code>)","text":"<p>This YAML file defines the \"team,\" which in this case is just a single agent. It defines the agent's prompt, gives it a web search tool, and configures the LLM.</p> <pre><code>name: \"simple_chat\"\ndescription: \"A simple chat example with user, assistant, search, and memory\"\n\nagents:\n  - name: \"assistant\"\n    description: \"Helpful AI assistant with search capabilities\"\n    prompt_template: \"prompts/assistant.md\"\n    tools: [\"web_search\"]\n    llm_config:\n      provider: \"deepseek\"\n      model: \"deepseek-chat\"\n\ntools:\n  - name: \"web_search\"\n    type: \"builtin\"\n</code></pre>"},{"location":"quickstart/#3-running-the-example","title":"3. Running the Example","text":"<p>Navigate to the <code>examples/simple_chat</code> directory and run the script:</p> <pre><code>cd examples/simple_chat\npython main.py\n</code></pre> <p>You can now have an interactive conversation with the assistant.</p>"},{"location":"quickstart/#example-2-simple-team-multi-agent-collaboration","title":"Example 2: Simple Team (Multi-Agent Collaboration)","text":"<p>This example showcases AgentX's multi-agent capabilities. A <code>Writer</code> agent drafts an article, and then a <code>Reviewer</code> agent provides feedback. The handoff between them is managed automatically by the <code>TaskExecutor</code> based on natural language conditions. It uses the \"fire-and-forget\" <code>execute_task</code> function.</p>"},{"location":"quickstart/#1-the-code-examplessimple_teammainpy","title":"1. The Code (<code>examples/simple_team/main.py</code>)","text":"<p>This script starts a task with a single prompt and streams the entire multi-agent collaboration to the console.</p> <pre><code>#!/usr/bin/env python3\nimport asyncio\nimport sys\nfrom pathlib import Path\n\nproject_root = Path(__file__).parent.parent.parent\nsys.path.insert(0, str(project_root / \"src\"))\n\nfrom agentx import execute_task\n\nasync def main():\n    config_path = str(Path(__file__).parent / \"config\" / \"team.yaml\")\n    prompt = \"Write a short article about remote work benefits.\"\n\n    async for update in execute_task(prompt, config_path, stream=True):\n        update_type = update.get(\"type\")\n\n        if update_type == \"content\":\n            print(update[\"content\"], end=\"\", flush=True)\n        elif update_type == \"handoff\":\n            print(f\"\\\\n\\\\n\ud83d\udd04 HANDOFF: {update['from_agent']} \u2192 {update['to_agent']}\\\\n\")\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"quickstart/#2-the-configuration-examplessimple_teamconfigteamyaml","title":"2. The Configuration (<code>examples/simple_team/config/team.yaml</code>)","text":"<p>This configuration defines the two agents (<code>writer</code> and <code>reviewer</code>) and, most importantly, the <code>handoffs</code> rules that govern their collaboration.</p> <pre><code>name: \"WriterReviewerTeam\"\nagents:\n  - name: writer\n    description: \"Professional content writer for creating high-quality articles\"\n    prompt_template: \"prompts/writer.md\"\n    llm_config:\n      model: deepseek/deepseek-chat\n\n  - name: reviewer\n    description: \"Quality assurance specialist for reviewing and improving content\"\n    prompt_template: \"prompts/reviewer.md\"\n    llm_config:\n      model: deepseek/deepseek-chat\n\n# Handoffs using natural language conditions\nhandoffs:\n  - from_agent: \"writer\"\n    to_agent: \"reviewer\"\n    condition: \"draft is complete and ready for review\"\n\n  - from_agent: \"reviewer\"\n    to_agent: \"writer\"\n    condition: \"feedback has been provided and revisions are needed\"\n</code></pre>"},{"location":"quickstart/#3-running-the-example_1","title":"3. Running the Example","text":"<p>Navigate to the <code>examples/simple_team</code> directory and run the script:</p> <pre><code>cd examples/simple_team\npython main.py\n</code></pre> <p>You will see the <code>Writer</code> generate a draft, followed by a <code>HANDOFF</code> message, and then the <code>Reviewer</code> providing its feedback.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides a detailed reference for AgentX's public API.</p>"},{"location":"api/#global-functions","title":"Global Functions","text":"<p>These are the primary entry points for interacting with AgentX.</p>"},{"location":"api/#agentx.execute_task","title":"<code>agentx.execute_task(prompt, config_path=None, stream=False)</code>  <code>async</code>","text":"<p>A convenience function to create and run a task in one call.</p> <p>This is a \"fire-and-forget\" method for autonomous runs.</p>"},{"location":"api/#agentx.start_task","title":"<code>agentx.start_task(prompt, config_path=None)</code>  <code>async</code>","text":"<p>A convenience function to create and start a task for interactive sessions.</p>"},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#agentx.core.agent.Agent","title":"<code>agentx.core.agent.Agent</code>","text":"<p>Represents an autonomous agent that manages its own conversation flow.</p> <p>Key Principles: - Each agent is autonomous and manages its own conversation flow - Agents communicate with other agents through public interfaces only - The brain is private to the agent - no external access - Tool execution is handled by orchestrator for security and control</p> <p>This combines: - AgentConfig (configuration data) - Brain (private LLM interaction) - Conversation management (delegates tool execution to orchestrator)</p>"},{"location":"api/#agentx.core.agent.Agent.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize agent with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>AgentConfig</code> <p>Agent configuration containing name, prompts, tools, etc.</p> required"},{"location":"api/#agentx.core.agent.Agent.add_tool","title":"<code>add_tool(tool)</code>","text":"<p>Add a tool to the agent's capabilities.</p>"},{"location":"api/#agentx.core.agent.Agent.build_system_prompt","title":"<code>build_system_prompt(context=None)</code>","text":"<p>Build the system prompt for the agent, including dynamic context and tool definitions.</p>"},{"location":"api/#agentx.core.agent.Agent.generate_response","title":"<code>generate_response(messages, system_prompt=None, orchestrator=None, max_tool_rounds=10)</code>  <code>async</code>","text":"<p>Generate a complete response with tool execution handled by orchestrator.</p> <p>This matches Brain's interface but includes tool execution loop.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Any]]</code> <p>Conversation messages in LLM format</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt override</p> <code>None</code> <code>orchestrator</code> <p>Orchestrator instance for tool execution</p> <code>None</code> <code>max_tool_rounds</code> <code>int</code> <p>Maximum tool execution rounds</p> <code>10</code> <p>Returns:</p> Type Description <code>str</code> <p>Complete response after all tool executions</p>"},{"location":"api/#agentx.core.agent.Agent.get_capabilities","title":"<code>get_capabilities()</code>","text":"<p>Get agent capabilities summary.</p>"},{"location":"api/#agentx.core.agent.Agent.get_tools_json","title":"<code>get_tools_json()</code>","text":"<p>Get the JSON schemas for the tools available to this agent.</p>"},{"location":"api/#agentx.core.agent.Agent.remove_tool","title":"<code>remove_tool(tool_name)</code>","text":"<p>Remove a tool from the agent's capabilities.</p>"},{"location":"api/#agentx.core.agent.Agent.reset_state","title":"<code>reset_state()</code>","text":"<p>Reset agent state.</p>"},{"location":"api/#agentx.core.agent.Agent.stream_response","title":"<code>stream_response(messages, system_prompt=None, orchestrator=None, max_tool_rounds=10)</code>  <code>async</code>","text":"<p>Stream response with tool execution handled by orchestrator.</p> <p>This matches Brain's interface but includes tool execution loop.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Dict[str, Any]]</code> <p>Conversation messages in LLM format</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt override</p> <code>None</code> <code>orchestrator</code> <p>Orchestrator instance for tool execution</p> <code>None</code> <code>max_tool_rounds</code> <code>int</code> <p>Maximum tool execution rounds</p> <code>10</code> <p>Yields:</p> Type Description <code>AsyncGenerator[str, None]</code> <p>Response chunks and tool execution status updates</p>"},{"location":"api/#agentx.core.agent.Agent.update_config","title":"<code>update_config(**kwargs)</code>","text":"<p>Update agent configuration.</p>"},{"location":"api/#agentx.core.orchestrator.Orchestrator","title":"<code>agentx.core.orchestrator.Orchestrator</code>","text":"<p>Central orchestrator for agent coordination and secure tool execution.</p> <p>This class handles: - Routing decisions between agents (core orchestration) - Dispatching tool calls to ToolExecutor for security - Managing team collaboration workflows - Intelligent handoff detection and execution</p>"},{"location":"api/#agentx.core.orchestrator.Orchestrator.__init__","title":"<code>__init__(team=None, max_rounds=None, timeout=None)</code>","text":"<p>Initialize orchestrator with team and limits.</p>"},{"location":"api/#agentx.core.orchestrator.Orchestrator.clear_execution_history","title":"<code>clear_execution_history()</code>","text":"<p>Clear tool execution history.</p>"},{"location":"api/#agentx.core.orchestrator.Orchestrator.decide_next_step","title":"<code>decide_next_step(current_agent, response, task_context)</code>  <code>async</code>","text":"<p>Core routing logic - decide what happens next.</p> <p>This is the primary orchestration responsibility.</p>"},{"location":"api/#agentx.core.orchestrator.Orchestrator.execute_single_tool","title":"<code>execute_single_tool(tool_name, agent_name='default', **kwargs)</code>  <code>async</code>","text":"<p>Dispatch single tool execution to ToolExecutor.</p> <p>Parameters:</p> Name Type Description Default <code>tool_name</code> <code>str</code> <p>Name of the tool to execute</p> required <code>agent_name</code> <code>str</code> <p>Name of the agent requesting execution</p> <code>'default'</code> <code>**kwargs</code> <p>Tool arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>ToolResult</code> <p>ToolResult with execution outcome</p>"},{"location":"api/#agentx.core.orchestrator.Orchestrator.execute_tool_calls","title":"<code>execute_tool_calls(tool_calls, agent_name='default')</code>  <code>async</code>","text":"<p>Dispatch tool calls to ToolExecutor for secure execution.</p> <p>This provides centralized security control over all tool execution.</p>"},{"location":"api/#agentx.core.orchestrator.Orchestrator.get_available_tools","title":"<code>get_available_tools(agent_name='default')</code>","text":"<p>Get list of tools available to an agent.</p>"},{"location":"api/#agentx.core.orchestrator.Orchestrator.get_execution_stats","title":"<code>get_execution_stats()</code>","text":"<p>Get tool execution statistics.</p>"},{"location":"api/#agentx.core.orchestrator.Orchestrator.get_tool_schemas_for_agent","title":"<code>get_tool_schemas_for_agent(agent_name='default')</code>","text":"<p>Get tool schemas available to a specific agent.</p>"},{"location":"api/#agentx.core.orchestrator.Orchestrator.route_to_agent","title":"<code>route_to_agent(agent_name, messages, system_prompt=None)</code>  <code>async</code>","text":"<p>Route messages to a specific agent for processing.</p> <p>The orchestrator delegates to the agent but provides itself for tool execution.</p>"},{"location":"api/#agentx.core.orchestrator.Orchestrator.stream_from_agent","title":"<code>stream_from_agent(agent_name, messages, system_prompt=None)</code>  <code>async</code>","text":"<p>Stream response from a specific agent.</p> <p>The orchestrator delegates to the agent but provides itself for tool execution.</p>"},{"location":"api/#agentx.core.task.Task","title":"<code>agentx.core.task.Task</code>","text":"<p>Primary interface for AgentX task execution.</p> One-shot execution <p>task = create_task(config_path) await task.execute_task(prompt)</p> Step-by-step execution <p>task = create_task(config_path) task.start_task(prompt) while not task.is_complete:     await task.step()</p>"},{"location":"api/#agentx.core.task.Task.__init__","title":"<code>__init__(team, task_id=None, workspace_dir=None)</code>","text":"<p>Initialize task with team configuration.</p>"},{"location":"api/#agentx.core.task.Task.add_artifact","title":"<code>add_artifact(name, content, metadata=None)</code>","text":"<p>Add an artifact to the task.</p>"},{"location":"api/#agentx.core.task.Task.add_step","title":"<code>add_step(step)</code>","text":"<p>Add a step to the conversation history.</p>"},{"location":"api/#agentx.core.task.Task.complete_task","title":"<code>complete_task()</code>","text":"<p>Mark the task as complete.</p>"},{"location":"api/#agentx.core.task.Task.execute_task","title":"<code>execute_task(prompt, initial_agent=None, stream=False)</code>  <code>async</code>","text":"<p>Execute task to completion (one-shot).</p>"},{"location":"api/#agentx.core.task.Task.pause_task","title":"<code>pause_task()</code>","text":"<p>Pause the task execution.</p>"},{"location":"api/#agentx.core.task.Task.resume_task","title":"<code>resume_task()</code>","text":"<p>Resume the task execution.</p>"},{"location":"api/#agentx.core.task.Task.set_current_agent","title":"<code>set_current_agent(agent_name)</code>","text":"<p>Set the current active agent.</p>"},{"location":"api/#agentx.core.task.Task.setup_storage_tools","title":"<code>setup_storage_tools()</code>","text":"<p>Setup storage tools for the task.</p>"},{"location":"api/#agentx.core.task.Task.start_task","title":"<code>start_task(prompt, initial_agent=None)</code>","text":"<p>Start task for step-by-step execution.</p>"},{"location":"api/#agentx.core.task.Task.step","title":"<code>step(user_input=None, stream=False)</code>  <code>async</code>","text":"<p>Execute one step (for step-by-step execution).</p>"},{"location":"api/#agentx.core.tool.ToolRegistry","title":"<code>agentx.core.tool.ToolRegistry</code>","text":"<p>Global tool registry that manages all available tools and creates schemas.</p> <p>This is a singleton that holds all registered tools and provides schema generation for any subset of tool names.</p>"},{"location":"api/#agentx.core.tool.ToolRegistry.clear","title":"<code>clear()</code>","text":"<p>Clears all registered tools. Primarily for testing.</p>"},{"location":"api/#agentx.core.tool.ToolRegistry.execute_tool","title":"<code>execute_tool(name, **kwargs)</code>  <code>async</code>","text":"<p>Execute a tool by name with automatic parameter validation.</p>"},{"location":"api/#agentx.core.tool.ToolRegistry.execute_tool_sync","title":"<code>execute_tool_sync(name, **kwargs)</code>","text":"<p>Synchronous wrapper for executing a tool. For use in non-async contexts.</p>"},{"location":"api/#agentx.core.tool.ToolRegistry.get_tool","title":"<code>get_tool(name)</code>","text":"<p>Get a tool, method, and pydantic model by name (for executor use).</p>"},{"location":"api/#agentx.core.tool.ToolRegistry.get_tool_schemas","title":"<code>get_tool_schemas(tool_names)</code>","text":"<p>Get detailed OpenAI function schemas for specified tools.</p>"},{"location":"api/#agentx.core.tool.ToolRegistry.list_tools","title":"<code>list_tools()</code>","text":"<p>List all registered tool names.</p>"},{"location":"api/#agentx.core.tool.ToolRegistry.register_tool","title":"<code>register_tool(tool)</code>","text":"<p>Register a tool and all its callable methods.</p>"},{"location":"arch/00-requirements/","title":"AgentX Framework: Core Requirements","text":"<ul> <li> <p>[x] 1.  File-Based Configuration: The entire multi-agent team structure and collaboration workflow are defined in and loaded from static, file-based configurations (e.g., <code>team.yaml</code>). These configurations serve as the complete, declarative definition of a team.</p> </li> <li> <p>[x] 2.  Orchestrator as Central Controller: The <code>Orchestrator</code> is the primary entry point and central controller for a task. It is responsible for managing the flow of the collaboration, not its content. Its duties include accepting the initial prompt, determining the next agent to act, dispatching tool calls, and terminating the collaboration based on defined conditions.</p> </li> <li> <p>[x] 3.  Team State Management: The framework maintains a persistent, file-based state for each task execution. This includes the conversation history, current agent, round count, artifacts, and other execution metadata stored in the task workspace.</p> </li> <li> <p>[x] 4.  Agent Prompt Templates: Each agent's behavior is defined by a Jinja2 template file that specifies its role, instructions, and context. The framework renders these templates with dynamic context (history, available tools, task prompt) to generate the agent's system prompt.</p> </li> <li> <p>[x] 5.  Comprehensive Data Structures: The framework uses well-defined Pydantic models for all data structures including TaskStep, ToolCall, ToolResult, Artifact, and various event types. These provide type safety, validation, and serialization.</p> </li> <li> <p>[x] 6.  Event-Driven Architecture: The framework emits structured events for all significant execution milestones (task start/complete, agent turns, tool calls, handoffs, errors) enabling monitoring, debugging, and integration with external systems.</p> </li> <li> <p>[x] 7.  Message Streaming Support: Separate from execution events, the framework supports real-time message streaming for UI updates, allowing clients to receive incremental content as it's generated by agents.</p> </li> <li> <p>[x] 8.  Workspace Management: Each task execution gets its own workspace directory containing state files, artifacts, history logs, and other task-specific data. This enables task persistence, resumption, and artifact management.</p> </li> <li> <p>[x] 9.  Tool Integration: The framework supports multiple tool types (builtin, python functions, MCP tools, HITL tools) with a unified interface. Tools are configured declaratively and can be shared across agents or restricted to specific agents.</p> </li> <li> <p>[x] 10. LLM Provider Abstraction: The framework abstracts LLM interactions through a unified interface supporting multiple providers (OpenAI, Anthropic, DeepSeek, Ollama, custom) with provider-specific optimizations and fallback mechanisms.</p> </li> <li> <p>[x] 11. Handoff Rules: Agent-to-agent handoffs are governed by explicit rules defined in the team configuration. These rules specify conditions, target agents, and handoff types (sequential, parallel) enabling sophisticated collaboration patterns.</p> </li> <li> <p>[x] 12. Memory Management: The framework provides both short-term (conversation context) and long-term (persistent) memory with semantic search capabilities. Memory operations are tracked and can be queried for debugging and optimization.</p> </li> <li> <p>[x] 13. Guardrails and Safety: Comprehensive safety mechanisms including input validation, output filtering, rate limiting, and content safety checks. Guardrail policies are configurable per agent and can block, warn, or log violations.</p> </li> <li> <p>[x] 14. Human-in-the-Loop (HITL): Built-in support for human intervention points through step-through execution mode, allowing users to pause execution, inspect state, modify context, and provide input at any point in the workflow.</p> </li> <li> <p>[x] 15. Step-Through Debugging: The framework supports step-by-step execution with breakpoints, allowing developers to pause execution, inspect state, modify context, and resume. Essential for development and debugging complex workflows.</p> </li> <li> <p>[ ] 16. Production Deployment: The framework includes deployment configurations, health checks, performance monitoring, and scaling capabilities for production environments including containerization and cloud deployment support.</p> </li> <li> <p>[x] 17. Favor DeepSeek Models: The framework defaults to DeepSeek models for reasoning and general scenarios, with easy configuration for other providers when needed.</p> </li> <li> <p>[x] 18. Autonomous Task Execution: The framework can execute tasks autonomously from start to completion, making intelligent decisions about agent handoffs, tool usage, and task termination without human intervention.</p> </li> <li> <p>[ ] 19. Advanced Workspace Management: Sophisticated workspace system with semantic search, automatic consolidation, and intelligent context management to maintain relevant information across long conversations.</p> </li> <li> <p>[x] 20. Rich Artifact Management: Advanced artifact handling including versioning, metadata tracking, cross-references, and support for various media types (code, documents, images, data).</p> </li> <li> <p>[ ] 21. Intelligent Context Management: Smart context compilation strategies, automatic summarization, and relevance-based filtering to optimize LLM context usage and maintain conversation coherence.</p> </li> </ul>"},{"location":"arch/01-architecture/","title":"AgentX System Architecture","text":""},{"location":"arch/01-architecture/#1-executive-summary","title":"1. Executive Summary","text":"<p>AgentX is an open-source backbone for building secure, observable, and fully autonomous multi-agent systems. A lightweight micro-kernel orchestrates specialised agents, turning a single user request into a coordinated workflow that spans tool execution, memory retrieval, and artifact management\u2014all within isolated, version-controlled workspaces. Every decision, message, and side effect is captured as a structured event, providing complete auditability and real-time insight into system behaviour.</p>"},{"location":"arch/01-architecture/#2-vision-principles","title":"2. Vision &amp; Principles","text":""},{"location":"arch/01-architecture/#21-project-vision","title":"2.1 Project Vision","text":"<p>AgentX enables organisations to decompose complex goals into collaborative Teams of agents, each focusing on a well-defined role. A central Orchestrator governs the conversation, selects the next agent to speak, and executes tools on the agents' behalf, while a Task Executor drives the lifecycle of the task itself. The result is a flexible framework that elevates individual agent capabilities into a cohesive, self-optimising system that can learn, adapt, and scale with minimal human intervention.</p>"},{"location":"arch/01-architecture/#22-architectural-principles","title":"2.2 Architectural Principles","text":"<p>The architecture rests on the following foundational principles:</p> <ul> <li>Separation of Concerns: Each subsystem has a single, well-defined responsibility, which reduces coupling and simplifies maintenance.</li> <li>Centralised Orchestration: A single Orchestrator governs coordination, security, and resource allocation, providing a uniform control plane.</li> <li>Agent Autonomy: Agents manage their own reasoning loops and private Brains, delegating only cross-cutting concerns upward.</li> <li>Event-Driven Coordination: Asynchronous, structured events enable scalable, loosely-coupled communication among subsystems.</li> <li>Configuration-Driven Behaviour: Teams, agents, and workflows are defined declaratively, allowing rapid iteration without code changes.</li> <li>Security by Design: All external interactions pass through audited, policy-enforced channels; least-privilege boundaries are maintained throughout.</li> <li>Workspace Isolation: Every task executes in its own version-controlled workspace, ensuring reproducibility and clean separation of artifacts.</li> </ul>"},{"location":"arch/01-architecture/#3-system-architecture-overview","title":"3. System Architecture Overview","text":"<pre><code>graph TD\n    %% Client Layer\n    CLI[CLI Interface]\n    API[REST / WebSocket API]\n\n    %% AgentX Core\n    TE[Task Executor]\n    ORCH[Orchestrator]\n    AGENTS[Team of Agents]\n    BRAIN[Agent Brain]\n    TOOL_EXEC[Tool Executor]\n    PLAT[Platform Services]\n\n    LLM[LLM Providers]\n    TOOLS[Builtin Tools]\n    API_EXT[External APIs]\n    MCP[MCP]\n\n    %% Vertical flow connections\n    CLI --&gt; TE\n    API --&gt; TE\n\n    TE --&gt; ORCH\n    TE --&gt; AGENTS\n    AGENTS --&gt; BRAIN\n    BRAIN --&gt; LLM\n\n    ORCH --&gt; TOOL_EXEC\n    TOOL_EXEC --&gt; TOOLS\n    TOOL_EXEC --&gt; MCP\n    MCP --&gt; API_EXT\n\n    TE --&gt; PLAT\n\n    %% Styling for rounded corners\n    classDef default stroke:#333,stroke-width:2px,rx:10,ry:10\n    classDef client stroke:#1976d2,stroke-width:2px,rx:10,ry:10\n    classDef core stroke:#388e3c,stroke-width:2px,rx:10,ry:10\n    classDef external stroke:#f57c00,stroke-width:2px,rx:10,ry:10\n    classDef platform stroke:#7b1fa2,stroke-width:2px,rx:10,ry:10\n\n    %% Apply styles to specific nodes\n    class CLI,API client\n    class TE,ORCH,AGENTS,BRAIN core\n    class LLM,API_EXT,MCP external\n    class TOOL_EXEC,TOOLS,PLAT platform</code></pre> <p>The AgentX architecture is composed of four distinct layers:</p> <ul> <li>Client Layer: Provides the primary user-facing interfaces, including a Command-Line Interface (CLI) for developers, a REST/WebSocket API for programmatic integration, and a web-based Dashboard for real-time monitoring.</li> <li>AgentX Core: Contains the essential components for task execution and agent collaboration. The <code>Task Executor</code> drives the workflow, the <code>Orchestrator</code> makes routing decisions, and the <code>Team of Agents</code> performs the reasoning.</li> <li>Platform Services: A suite of shared, pluggable services that support the core. These include the secure <code>Tool Executor</code>, <code>Configuration System</code>, <code>Event Bus</code>, and stateful services for <code>Memory</code> and <code>Observability</code>.</li> <li>External Systems: Represents all external dependencies, such as <code>LLM Providers</code>, <code>Vector Databases</code> for memory, <code>Git</code> for workspace versioning, and any third-party <code>APIs</code> that tools may call.</li> </ul>"},{"location":"arch/01-architecture/#4-collaboration-model","title":"4. Collaboration Model","text":"<p>In AgentX, a Team of collaborating agents is the primary mechanism for executing complex tasks. The core runtime consists of three key components that work in concert to manage the task lifecycle.</p>"},{"location":"arch/01-architecture/#41-key-roles","title":"4.1 Key Roles","text":"<ul> <li>Task Executor: Owns the end-to-end lifecycle of a single task. It acts as the primary workflow engine, responsible for provisioning the workspace, managing the overall task state, and deciding which agent to invoke in each turn of the conversation.</li> <li>Orchestrator: Acts as a dedicated, centralized service for tool management. It does not route between agents. Instead, it is invoked by an agent when it needs to run a tool. The Orchestrator's sole responsibilities are to validate tool-call requests against their schemas, dispatch them to the secure <code>ToolExecutor</code>, and return the results to the calling agent.</li> <li>Agent: Encapsulates a specialised role (e.g., researcher, writer). It receives control from the <code>Task Executor</code>, reasons with its private Brain, and can invoke the <code>Orchestrator</code> if it needs to execute a tool.</li> </ul>"},{"location":"arch/01-architecture/#42-execution-modes","title":"4.2 Execution Modes","text":"<p>AgentX supports two primary modes of execution, offering a trade-off between autonomy and control.</p> <p>1. Autonomous Execution (<code>execute_task</code>)</p> <p>This \"fire-and-forget\" mode is ideal for production. A client submits a task and waits for a final result, while the <code>Task Executor</code> runs the entire multi-agent collaboration autonomously.</p> <pre><code>sequenceDiagram\n    participant C as Client\n    participant TX as Task Executor\n    participant AG as Agent\n    participant OR as Orchestrator\n    participant TE as Tool Executor\n\n    C-&gt;&gt;TX: execute_task(prompt)\n    loop Autonomous Loop\n        TX-&gt;&gt;AG: invoke_agent\n        AG--&gt;&gt;OR: request_tool_execution\n        OR--&gt;&gt;TE: dispatch_tool\n        TE--&gt;&gt;OR: return_result\n        OR--&gt;&gt;AG: provide_result\n        AG--&gt;&gt;TX: return_turn_response\n    end\n    TX--&gt;&gt;C: final_result</code></pre> <p>2. Interactive Execution (<code>start_task</code> &amp; <code>step</code>)</p> <p>For debugging or human-in-the-loop workflows, a client can call <code>start_task</code> to get a <code>Task</code> object, then repeatedly call <code>step()</code> to advance the execution one turn at a time.</p> <pre><code>sequenceDiagram\n    participant C as Client\n    participant TX as Task Executor\n    participant AG as Agent\n    participant OR as Orchestrator\n    participant TE as Tool Executor\n\n    C-&gt;&gt;TX: start_task(prompt)\n    TX--&gt;&gt;C: task_object\n\n    loop Interactive Steps\n        C-&gt;&gt;TX: task.step()\n        TX-&gt;&gt;AG: invoke_agent\n        AG--&gt;&gt;OR: request_tool_execution\n        OR--&gt;&gt;TE: dispatch_tool\n        TE--&gt;&gt;OR: return_result\n        OR--&gt;&gt;AG: provide_result\n        AG--&gt;&gt;TX: return_turn_response\n        TX--&gt;&gt;C: step_result\n    end</code></pre>"},{"location":"arch/01-architecture/#5-agent-internals","title":"5. Agent Internals","text":"<p>While the <code>Task Executor</code> manages the high-level conversation flow between agents, each <code>Agent</code> is responsible for its own internal reasoning process. This process involves hydrating a prompt template with runtime context, executing a \"monologue\" with its private Brain, and optionally streaming its response back to the client.</p>"},{"location":"arch/01-architecture/#51-prompt-templating","title":"5.1 Prompt Templating","text":"<p>An agent's core behavior and persona are defined by its system prompt, which is typically loaded from a Jinja2 template file. Before the <code>Task Executor</code> invokes an agent, it injects dynamic context into this template. This ensures the agent is fully aware of the current state of the task. Common context variables include:</p> <ul> <li>The full conversation history.</li> <li>A list of available tools and their JSON schemas.</li> <li>The initial task objective and any user-provided parameters.</li> <li>Summaries or references to artifacts in the workspace.</li> </ul> <p>This just-in-time templating allows agents to be both powerful and reusable, adapting their behavior to the specific needs of each task.</p>"},{"location":"arch/01-architecture/#52-internal-monologue-and-tool-calling-loop","title":"5.2 Internal Monologue and Tool-Calling Loop","text":"<p>When an agent is invoked by the <code>Task Executor</code>, it enters a sophisticated internal loop to produce the most informed output possible. This loop includes a crucial self-correction mechanism for tool usage.</p> <ol> <li>Initial Reasoning: The <code>Agent</code> passes the prompt to its <code>Brain</code> (LLM), which generates an initial response that may include one or more desired tool calls.</li> <li>Request Execution: The <code>Agent</code> requests execution for these tool calls from the <code>Orchestrator</code>.</li> <li>Validation and Self-Correction: The <code>Orchestrator</code> validates the calls against the registered tool schemas.<ul> <li>If a call is invalid, the <code>Orchestrator</code> immediately returns a structured validation error. The <code>Agent</code> then passes this error back to the <code>Brain</code>, which attempts to generate a corrected tool call. This loop continues until the call is valid.</li> </ul> </li> <li>Secure Dispatch: Once a tool call is valid, the <code>Orchestrator</code> dispatches it to the <code>ToolExecutor</code>, which runs the tool in a secure sandbox.</li> <li>Result Integration: The results are returned up the chain to the <code>Agent</code>, which gives them to the <code>Brain</code>. The <code>Brain</code> uses this new information to generate its final, grounded response for the turn.</li> <li>Final Response: The <code>Agent</code> returns its final response to the <code>Task Executor</code>, completing its turn.</li> </ol> <p>This entire process ensures that agents can autonomously use tools, correct their own mistakes, and ground their reasoning in real-world information before finalizing their output.</p> <pre><code>sequenceDiagram\n    participant TaskExecutor\n    participant Agent\n    participant Brain\n    participant Orchestrator\n    participant ToolExecutor\n\n    TaskExecutor-&gt;&gt;Agent: invoke\n    Agent-&gt;&gt;Brain: generate response\n    Brain--&gt;&gt;Agent: draft response (with tool call)\n\n    loop Tool Self-Correction\n        Agent-&gt;&gt;Orchestrator: request execution\n        alt Invalid Tool Call\n            Orchestrator--&gt;&gt;Agent: validation error\n            Agent-&gt;&gt;Brain: correct tool call\n            Brain--&gt;&gt;Agent: corrected tool call\n        else Valid Tool Call\n            Orchestrator-&gt;&gt;ToolExecutor: dispatch tool\n            ToolExecutor--&gt;&gt;Orchestrator: tool result\n            Orchestrator--&gt;&gt;Agent: return result\n\n            Agent-&gt;&gt;Brain: generate final response\n            Brain--&gt;&gt;Agent: final response\n            break\n        end\n    end\n\n    Agent--&gt;&gt;TaskExecutor: return final response\nend</code></pre>"},{"location":"arch/01-architecture/#53-end-to-end-streaming","title":"5.3 End-to-End Streaming","text":"<p>To provide maximum transparency and a highly responsive user experience, AgentX is designed for end-to-end streaming. This is more than just streaming the final answer; it means that every significant piece of text generated during the task lifecycle is yielded back to the client in real-time.</p> <p>This is achieved by making streaming the default behavior at every layer of the stack:</p> <ul> <li>Brain: The <code>Brain</code> streams token-by-token output directly from the underlying LLM provider.</li> <li>Agent: The <code>Agent</code> streams its internal monologue, including its reasoning process and its decision to call tools.</li> <li>Task Executor: The <code>Task Executor</code> orchestrates these streams, interleaving agent monologues with tool execution status messages and final outputs.</li> </ul> <p>This architecture allows a developer or user to watch the entire multi-agent collaboration unfold in real-time, offering unparalleled insight for debugging, monitoring, and human-in-the-loop interaction. It transforms the \"black box\" of agent reasoning into a transparent, observable process.</p>"},{"location":"arch/01-architecture/#6-state-and-context-management","title":"6. State and Context Management","text":"<p>AgentX is designed around the core principle that agents should operate on a rich, durable, and easily accessible context. This is achieved through two tightly integrated components: the Workspace and the Memory System.</p>"},{"location":"arch/01-architecture/#61-workspace-a-durable-foundation","title":"6.1 Workspace: A Durable Foundation","text":"<p>The Workspace is the stateful heart of every task. It is a version-controlled directory that provides the foundation for iterative development, task resumption, and human-in-the-loop collaboration. By persisting every message, artifact, and state change to the workspace, AgentX guarantees full auditability and allows tasks to be paused, inspected, modified, and resumed at any point.</p> <p>Every workspace contains:</p> <ul> <li>A complete, append-only log of the conversation history (<code>history.jsonl</code>).</li> <li>A version-controlled <code>artifacts</code> directory where all agent outputs (code, documents, data) are stored.</li> <li>A <code>state.json</code> file capturing the latest state of the Task Executor and all agents.</li> </ul> <p>This robust state management is what enables developers to treat tasks not as ephemeral processes, but as durable, long-running workflows that can be debugged, refined, and improved over time.</p>"},{"location":"arch/01-architecture/#62-memory-intelligent-context-retrieval","title":"6.2 Memory: Intelligent Context Retrieval","text":"<p>The Memory acts as the intelligent, unified gateway for retrieving contextual data from the Workspace. It is more than a simple wrapper around a vector database; it is the sole entry point for agents to perform intelligent data fetching, ensuring they have the most relevant information without exceeding token limits.</p> <p>Its responsibilities are twofold:</p> <ol> <li>Context Ingestion: It automatically captures and indexes conversational history, agent-generated artifacts, and other designated data sources from the Workspace into a long-term, searchable store.</li> <li>Intelligent Retrieval: It provides a simple query interface for agents to retrieve contextually relevant information. The system handles the complexity of searching across different data types and uses semantic ranking to return only the most salient facts.</li> </ol> <p>By abstracting away the complexities of data storage and retrieval, the Memory System allows agents to remain focused on reasoning, while ensuring their prompts are always grounded with high-quality, relevant context from the Workspace.</p>"},{"location":"arch/01-architecture/#63-example-scenario-context-aware-writing","title":"6.3 Example Scenario: Context-Aware Writing","text":"<p>To illustrate how these components work together, consider a <code>Writer</code> agent tasked with drafting a report. A <code>Researcher</code> agent has already run, populating the <code>Workspace</code> with dozens of source documents.</p> <ol> <li>Initial State: The <code>Workspace</code> contains all source documents as artifacts (e.g., <code>source_01.txt</code>, <code>source_02.txt</code>, etc.). The <code>Memory System</code> has indexed the content of each of these sources. The main report is still empty.</li> <li>Writing the First Section: The <code>Writer</code> is tasked with \"Write the introduction.\" It queries the <code>Memory System</code>: \"Find sources relevant to the overall topic.\" The <code>Memory System</code> returns the most relevant source documents. The <code>Writer</code> uses them to draft the introduction, which is then saved back to the <code>Workspace</code> as <code>report_v1.md</code>.</li> <li>Preventing Redundancy: When the <code>Writer</code> is next tasked with \"Write the 'History of AI' section,\" it performs a more sophisticated query: \"Find sources related to 'the history of AI' that are not already referenced in <code>report_v1.md</code>.\"</li> <li>Intelligent Retrieval: The <code>Memory System</code> understands this query. It performs a semantic search for \"history of AI\" across all source documents, but it also performs a negative semantic search, filtering out any sources whose content closely matches what is already in <code>report_v1.md</code>.</li> <li>Grounded Response: The <code>Memory System</code> returns a fresh, relevant, and unused set of sources. The <code>Writer</code> can now draft the new section with confidence, knowing it is not repeating information.</li> </ol> <p>This scenario demonstrates how the combination of a durable <code>Workspace</code> and an intelligent <code>Memory System</code> enables agents to perform complex, stateful tasks that would be impossible with a simple conversational context window.</p>"},{"location":"arch/01-architecture/#7-platform-services","title":"7. Platform Services","text":"<p>Platform Services provide common capabilities that sit outside the tight execution loop yet remain essential to every task. They are deployed as shared, multi-tenant components and accessed via well-defined APIs, allowing the core runtime to stay lightweight while still benefiting from robust storage, configuration, and monitoring facilities.</p>"},{"location":"arch/01-architecture/#71-configuration-system","title":"7.1 Configuration System","text":"<p>The Configuration System is the single source of truth for teams, agents, tools, and runtime policies. It loads declarative YAML files, validates them against versioned schemas, and exposes the resulting objects to the Task Executor and Orchestrator at startup or on hot-reload.</p>"},{"location":"arch/01-architecture/#72-event-bus","title":"7.2 Event Bus","text":"<p>All significant actions in the framework emit structured events that flow through the Event Bus. This design decouples producers from consumers, enabling real-time monitoring, auditing, and external integrations without burdening the hot code-path.</p>"},{"location":"arch/01-architecture/#73-memory-system-infrastructure","title":"7.3 Memory System Infrastructure","text":"<p>While Section 6 described the Memory System from a business logic perspective, this section focuses on its infrastructure implementation. The Memory System uses a pluggable backend architecture that abstracts away the complexities of different memory storage implementations.</p> <p>Implementation Architecture:</p> <ul> <li>Memory Backend Interface: A standardized interface that allows different memory providers (mem0, vector databases, etc.) to be plugged in seamlessly</li> <li>mem0 Integration: The default backend leverages mem0 for intelligent memory management, providing automatic memory extraction, storage, and retrieval</li> <li>Vector Database Support: Direct integration with vector databases for semantic search capabilities</li> <li>Memory Factory Pattern: A factory that instantiates the appropriate memory backend based on configuration</li> </ul> <p>Key Infrastructure Features:</p> <ul> <li>Automatic Indexing: Background processes that continuously index new content from workspaces</li> <li>Scalable Storage: Support for both local and distributed memory storage backends</li> <li>Memory Lifecycle Management: Automatic cleanup, archival, and optimization of stored memories</li> <li>Provider Abstraction: Clean separation between the memory interface and underlying storage technology</li> </ul> <p>This infrastructure design ensures that the Memory System can scale from development environments using local storage to production deployments using enterprise-grade vector databases, all without changing the business logic layer.</p>"},{"location":"arch/01-architecture/#74-observability","title":"7.4 Observability","text":"<p>The Observability service provides real-time insight into the inner workings of the AgentX framework. It subscribes to the <code>Event Bus</code> to receive a live stream of all system events\u2014from task creation to agent handoffs to tool executions. This data is then exposed through a built-in web dashboard, allowing developers to:</p> <ul> <li>Visually trace the entire lifecycle of a task.</li> <li>Inspect the contents of any agent's workspace, including conversation history and artifacts.</li> <li>Debug complex multi-agent interactions in real-time.</li> </ul> <p>By making the system transparent by default, the Observability service dramatically reduces the time required to build, test, and refine sophisticated agent-based workflows.</p>"},{"location":"arch/01-architecture/#8-builtin-tools","title":"8. Builtin Tools","text":"<p>Builtin Tools extend AgentX with first-class capabilities without requiring users to write custom plugins. They are registered automatically at startup and are available to any agent subject to security policy. These tools are designed to be general-purpose and cover the most common needs of autonomous agents.</p> <p>Key builtin tools include:</p> <ul> <li>Storage Tools: Clean file operations that use the storage layer, providing secure workspace-scoped file management with proper isolation and version control. Includes <code>read_file</code>, <code>write_file</code>, <code>list_directory</code>, <code>delete_file</code>, and other essential file operations.</li> <li>Web Tools: Advanced web capabilities including <code>web_search</code>, <code>extract_content</code> for web scraping, and <code>automate_browser</code> for AI-driven browser automation using natural language instructions.</li> <li>Memory Tools: Direct interface to the Memory system for storing and retrieving contextual information across task sessions.</li> <li>Context Tools: Tools for managing and updating task context variables, allowing agents to maintain and share state information.</li> <li>Planning Tools: Sophisticated task planning and execution tracking tools that help agents break down complex tasks into manageable phases.</li> <li>Search Tools: Web search capabilities using multiple search engines (Google, Bing, DuckDuckGo) with configurable parameters.</li> </ul> <p>By providing these foundational capabilities, AgentX ensures that developers can build powerful and effective agent teams from day one.</p>"},{"location":"arch/01-architecture/#9-extensibility","title":"9. Extensibility","text":"<p>AgentX is designed as a production-ready backbone, which means it must be extensible at every layer.</p> <ul> <li>Adding LLM Providers: The <code>Brain</code> component uses a provider model that allows new LLMs to be integrated by implementing a simple, common interface.</li> <li>Custom Tools: Developers can add their own tools by decorating Python functions. These are registered alongside builtin tools and exposed to agents in the same way.</li> <li>MCP Tools &amp; API Integration: For complex integrations, the framework supports Multi-Container Platform (MCP) tools, which run in secure sandboxes, allowing safe interaction with enterprise APIs and SaaS platforms.</li> </ul>"},{"location":"arch/01-architecture/#10-the-future-of-agentx","title":"10. The Future of AgentX","text":"<p>AgentX will continue to evolve along three pillars: performance, interoperability, and safety. Upcoming priorities include native support for streaming multimodal models, tighter integration with external knowledge graphs to improve grounding, and progressive guardrail policies that adapt to organisational compliance requirements. We also plan to introduce a plug-and-play planning module so that teams can experiment with alternative orchestration strategies without modifying core code. Finally, deeper observability hooks\u2014spanning trace-level token accounting through to high-level outcome metrics\u2014will help users fine-tune cost, latency, and quality trade-offs.</p>"},{"location":"arch/02-state-and-context/","title":"State and Context: Enabling Complex, Long-Running Tasks","text":"<p>A key challenge in building autonomous AI systems is managing state. For an agent to perform complex, multi-step tasks like writing a novel or developing a software project, it needs a reliable, long-term memory of its work. This document outlines AgentX's state and context architecture, which is designed specifically to address the hard problems of contextual understanding in large-scale tasks.</p>"},{"location":"arch/02-state-and-context/#1-the-core-challenges-of-stateful-ai","title":"1. The Core Challenges of Stateful AI","text":"<p>Before detailing the architecture, it's crucial to understand the specific challenges AgentX aims to solve. Effective state and context management must address the following:</p> <ul> <li> <p>Navigating Large and Complex Workspaces: A significant project, whether it's a codebase or a research archive, can contain hundreds or thousands of files. An agent cannot simply \"list all files\" and be effective. It needs a way to understand the project's structure, identify key artifacts, and navigate the workspace intelligently without being overwhelmed.</p> </li> <li> <p>Processing Long-Form Content: A single artifact, like a legacy source code file or a detailed technical paper, can be thousands of lines long\u2014far exceeding the context window of any LLM. An agent tasked with refactoring a large file or summarizing a dense document cannot simply \"read the file.\" It needs a mechanism to process content in a way that preserves the overall structure and meaning.</p> </li> <li> <p>Contextual Scoping and Filtering: Agents often start with a vast pool of raw data, such as gigabytes of financial reports for a market analysis. The critical challenge is to narrow down this \"firehose of information\" to a small, relevant subset of data that is actually valuable for the task at hand. An agent writing a research summary needs to intelligently select its sources, not just process all of them.</p> </li> <li> <p>Balancing Performance with Quality: While quality is paramount, system performance cannot be ignored. A system that is perfectly thorough but takes hours to respond is impractical. We need an architecture that provides high-quality, relevant context to the agent in a timely manner, making smart trade-offs between exhaustive search and responsive interaction.</p> </li> <li> <p>Automated Quality Assessment: How does the system know if a generated artifact is \"good\"? For code, this could mean passing linting checks, compiling successfully, or passing unit tests. For a document, it might mean checking for spelling errors or grammatical correctness. The system needs a mechanism to automatically assess the quality of its own outputs to guide the iterative process.</p> </li> </ul>"},{"location":"arch/02-state-and-context/#2-the-architecture-a-two-layered-solution","title":"2. The Architecture: A Two-Layered Solution","text":"<p>AgentX addresses these challenges with a two-layered architecture: The Workspace provides a durable, comprehensive record of the task, while the Memory System provides the intelligent interface for navigating and understanding it.</p>"},{"location":"arch/02-state-and-context/#21-layer-1-the-workspace-a-durable-version-controlled-record","title":"2.1. Layer 1: The Workspace - A Durable, Version-Controlled Record","text":"<p>The Workspace is the single source of truth for a task. By creating a version-controlled directory containing every message, artifact, and state change, it directly addresses the need for a complete and traceable history.</p> <ul> <li>Solution for Navigating Large Workspaces: While the Workspace stores everything, agents interact with it via tools. Tools like <code>list_directory</code> combined with an agent's reasoning allow it to explore the structure of the workspace step-by-step, rather than being flooded with a list of thousands of files at once.</li> <li>Foundation for Quality Assessment: By providing a stable location for artifacts (e.g., <code>main.py</code>), the Workspace enables the creation of quality assessment tools. A <code>linting_tool</code> or <code>build_tool</code> can be pointed at artifacts in the Workspace to check their integrity, providing structured feedback that the agent can act upon.</li> </ul>"},{"location":"arch/02-state-and-context/#22-layer-2-the-memory-system-an-intelligent-knowledge-base","title":"2.2. Layer 2: The Memory System - An Intelligent Knowledge Base","text":"<p>The Memory System is not merely a passive query layer but a sophisticated, multi-faceted knowledge base that sits on top of the Workspace. It proactively transforms the raw, version-controlled data into actionable intelligence for the agents. Its responsibilities are threefold:</p> <ul> <li> <p>1. Semantic Indexing and Retrieval (Cold Data Querying): This is the foundational capability. The Memory System indexes the entire contents of the Workspace, allowing agents to perform fast, semantic searches over vast amounts of \"cold\" data. This solves the problem of processing long-form content by allowing an agent to find relevant snippets (e.g., \"Find functions in services.py related to payment processing\") without loading entire files into its context window.</p> </li> <li> <p>2. Intelligent Summarization and Abstraction: The Memory System can synthesize information to provide high-level, abstract summaries on demand. For example, an agent can ask it to generate a \"virtual README\" for a directory it has never seen before, or to summarize the key decisions from a long conversation history. This allows agents to quickly orient themselves in complex environments without needing to read every single line of source material.</p> </li> <li> <p>3. Proactive Knowledge Synthesis (Rules, Constraints, and Hot-Issues): This is the system's most advanced function, where it acts as a true cognitive partner. It goes beyond analyzing workspace data to internalize user instructions and preferences, creating a set of rules and guardrails for the agent's behavior.</p> </li> <li>Capturing User Constraints: The system is designed to capture and enforce high-level user constraints that persist across interactions. If a user states, \"When writing about Tesla's future, never mention Elon Musk,\" the Memory System records this as a core project requirement. This rule is then used to guide content generation and validation, ensuring the final output adheres to the user's explicit directive.</li> <li>Learning from Feedback: It learns from iterative feedback to codify developer or user preferences. When a developer says, \"You should never create a <code>requirements.txt</code> file; use <code>pyproject.toml</code> instead,\" the Memory System flags this as a permanent rule for all future file operations in that workspace. This prevents the agent from repeating mistakes or asking redundant questions.</li> <li>Tracking Hot-Issues: It maintains a \"working memory\" of transient problems. For instance, if a unit test fails, the Memory System flags it as a \"hot issue\" that is automatically surfaced in the agent's context on every subsequent step until it is resolved. This prevents critical blockers from being lost in a long history.</li> </ul>"},{"location":"arch/02-state-and-context/#3-example-workflow-revisited-solving-problems-in-practice","title":"3. Example Workflow Revisited: Solving Problems in Practice","text":"<p>Consider the workflow of an agent tasked with adding a feature to a large, existing codebase, but this time using the full power of the intelligent Memory System.</p> <ol> <li>Challenge - Capturing Constraints: The user starts by saying, \"I'm getting a strange bug related to caching, so for now, please add a <code>@disable_cache</code> decorator to any new data-fetching functions you write.\" The Memory System logs <code>Rule: Must add '@disable_cache' to new data-fetching functions.</code></li> <li>Challenge - Navigating the Workspace: The agent doesn't know the code structure. It asks, <code>summarize_directory_purpose(path='/src/api')</code>. The Memory System synthesizes a description, and also surfaces the active rule about caching.</li> <li>Challenge - Processing Long-Form Content: The agent identifies a large <code>services.py</code> file and uses <code>search_memory(query='Find functions in services.py related to user data')</code> to get the relevant snippets.</li> <li>Challenge - Quality Assessment &amp; Hot-Issue Tracking: After writing a new function to the file, the agent calls <code>run_unit_tests_tool()</code>. The test fails. The Memory System detects this and creates a high-priority \"hot issue\": <code>Critical: Unit test 'test_new_feature' is failing.</code></li> <li>Focused Iteration: On the agent's next reasoning cycle, two items are injected into its context: the Rule about the decorator and the Hot-Issue about the failing test. The agent sees the code it wrote is missing the decorator. It adds it, confident this will fix the issue.</li> <li>Resolution: The agent runs the tests again. They pass. The Memory System sees the success event, automatically resolves and removes the \"hot issue,\" and the task proceeds. The rule about the decorator remains active.</li> </ol> <p>This workflow, grounded in a state and context architecture powered by an intelligent knowledge base, demonstrates how AgentX provides a practical and scalable solution to the core challenges of building sophisticated, stateful AI agents.</p>"},{"location":"arch/02-state-and-context/#4-system-design-and-implementation","title":"4. System Design and Implementation","text":"<p>This section details the technical implementation of the Workspace and Memory System, explaining how the conceptual goals are achieved.</p>"},{"location":"arch/02-state-and-context/#41-workspace-design","title":"4.1. Workspace Design","text":"<p>The Workspace is not a conceptual space but a physical directory on the filesystem, managed by a <code>GitStorage</code> backend.</p> <ul> <li>Core Technology: Every task workspace is a Git repository. This provides a robust, built-in mechanism for versioning, change tracking, and durability.</li> <li>Directory Structure:</li> <li><code>artifacts/</code>: This subdirectory contains all the files created or modified by agents (e.g., source code, documents, data files). This is the tangible work product of the system.</li> <li><code>history.jsonl</code>: This file is the immutable, ground-truth log of the task. Every single event\u2014every user message, agent thought process, tool call, and tool result\u2014is appended to this file as a JSON object. The system uses this for auditing, debugging, and for rebuilding the state of the Memory system if needed.</li> <li>Interaction Model: Agents do not directly execute <code>git</code> commands. They interact with the workspace via a clean abstraction layer: the <code>Storage</code> tools (<code>read_file</code>, <code>write_file</code>, <code>list_directory</code>). After an agent successfully executes a <code>write_file</code> operation, the <code>GitStorage</code> backend automatically commits the change to the repository with a descriptive message, creating a precise, versioned history of the work.</li> </ul>"},{"location":"arch/02-state-and-context/#42-memory-system-design","title":"4.2. Memory System Design","text":"<p>The Memory System is an active component that hooks into the core event loop of the system to process information and provide context.</p> <ul> <li> <p>Core Components:</p> </li> <li> <p>Event Bus Listener: The Memory System subscribes to the central <code>EventBus</code>. This allows it to receive a real-time stream of all events happening within the system (e.g., <code>UserMessageSent</code>, <code>ToolCallExecuted</code>, <code>AgentThoughtProcess</code>).</p> </li> <li>Pluggable Memory Backend: This is the storage engine for memory objects. It can be a vector database (like <code>Mem0</code>) for semantic search, a relational database, or a simple file-based store. It's responsible for persisting and retrieving the memory objects created by the Synthesis Engine.</li> <li> <p>Synthesis Engine: This is the logical core of the Memory System. It's a service that contains the business logic for creating and managing memories. It listens to events from the Event Bus and decides what actions to take.</p> </li> <li> <p>Data and Control Flow:</p> </li> <li>Indexing Content: When the Event Bus Listener sees a <code>ToolCallExecuted</code> event for a <code>write_file</code> operation, it triggers the Synthesis Engine. The engine, in turn, takes the content of the written file and instructs the Memory Backend to index it for future semantic search.</li> <li>Creating Rules and Hot-Issues:<ul> <li>When a <code>UserMessageSent</code> event occurs, the Synthesis Engine can make an LLM call to analyze the text for imperative commands or constraints. If a constraint like \"Don't use <code>requirements.txt</code>\" is found, it creates a <code>CONSTRAINT</code> memory object and saves it to the backend.</li> <li>When a <code>ToolCallExecuted</code> event occurs with a <code>status: FAILED</code> and is identified as a critical failure (e.g., from a test runner tool), the Synthesis Engine creates a <code>HOT_ISSUE</code> memory object.</li> </ul> </li> <li>Injecting Context: This is the critical retrieval step. Before the <code>Orchestrator</code> runs an agent's reasoning cycle, it makes a call to the Memory System: <code>memory.get_relevant_context(last_user_message)</code>. The Memory System's implementation of this method is designed to always retrieve all active <code>CONSTRAINT</code> and <code>HOT_ISSUE</code> objects from its backend, in addition to performing a semantic search on the user's message. This combined payload is what gets inserted into the agent's prompt, ensuring it is always aware of the most critical rules and problems.</li> </ul>"},{"location":"arch/02-state-and-context/#43-system-architecture-diagram","title":"4.3. System Architecture Diagram","text":"<p>The following diagram illustrates the data flow for how the Memory System processes events and provides context back to the agent.</p> <pre><code>graph TD\n    subgraph \"External Systems\"\n        User\n        Orchestrator\n        EventBus\n    end\n\n    subgraph \"Memory System\"\n        MSL[Event Bus Listener]\n        MSE[Synthesis Engine &lt;br/&gt; (LLM-Powered Logic)]\n        MSB[Memory Backend &lt;br/&gt; (Vector DB + KV Store)]\n    end\n\n    User -- \"Sends Message\" --&gt; EventBus\n    Orchestrator -- \"Executes Tool\" --&gt; EventBus\n\n    EventBus -- \"1. Receives Events &lt;br/&gt; (UserMessage, ToolResult)\" --&gt; MSL\n    MSL -- \"2. Forwards Events\" --&gt; MSE\n\n    MSE -- \"3a. Analyzes for Constraints\" --&gt; MSE\n    MSE -- \"3b. Analyzes for Hot-Issues\" --&gt; MSE\n    MSE -- \"3c. Extracts Content for Indexing\" --&gt; MSE\n\n    MSE -- \"4. Persists Memories\" --&gt; MSB\n    MSB -- \"Stores: &lt;br/&gt; - Constraints &lt;br/&gt; - Hot-Issues &lt;br/&gt; - Vector Embeddings\" --&gt; MSB\n\n    Orchestrator -- \"5. get_relevant_context()\" --&gt; MSE\n    MSE -- \"6. Queries Backend\" --&gt; MSB\n    MSB -- \"7. Returns Memories\" --&gt; MSE\n    MSE -- \"8. Injects into Agent Prompt\" --&gt; Orchestrator</code></pre>"},{"location":"arch/02-state-and-context/#5-detailed-implementation-blueprint","title":"5. Detailed Implementation Blueprint","text":"<p>This section provides implementation-level details for the key components.</p>"},{"location":"arch/02-state-and-context/#51-memory-object-data-models","title":"5.1. Memory Object Data Models","text":"<p>The system uses strongly-typed data models to represent different types of memories. These can be implemented as Pydantic models or similar data classes.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Literal, Optional\nfrom uuid import UUID, uuid4\n\nclass Memory(BaseModel):\n    id: UUID = Field(default_factory=uuid4)\n    type: Literal[\"CONSTRAINT\", \"HOT_ISSUE\", \"DOCUMENT_CHUNK\"]\n    content: str\n    source_event_id: Optional[UUID] = None\n    is_active: bool = True # Used to resolve hot-issues\n\nclass Constraint(Memory):\n    type: Literal[\"CONSTRAINT\"] = \"CONSTRAINT\"\n    # e.g., \"Do not use requirements.txt\"\n\nclass HotIssue(Memory):\n    type: Literal[\"HOT_ISSUE\"] = \"HOT_ISSUE\"\n    # e.g., \"Unit test 'test_payment_flow' is failing.\"\n\nclass DocumentChunk(Memory):\n    type: Literal[\"DOCUMENT_CHUNK\"] = \"DOCUMENT_CHUNK\"\n    source_file_path: str\n    # e.g., A chunk of text from a file.\n</code></pre>"},{"location":"arch/02-state-and-context/#52-core-component-interfaces-python-like-pseudocode","title":"5.2. Core Component Interfaces (Python-like Pseudocode)","text":"<p>These interfaces define the contracts between the major components.</p> <pre><code>from abc import ABC, abstractmethod\n\n# --- Backend Interface ---\nclass MemoryBackend(ABC):\n    @abstractmethod\n    def save_memories(self, memories: list[Memory]):\n        \"\"\"Persists a list of memory objects.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_active_rules(self) -&gt; list[Memory]:\n        \"\"\"Returns all active CONSTRAINT and HOT_ISSUE memories.\"\"\"\n        pass\n\n    @abstractmethod\n    def search_documents(self, query: str, top_k: int) -&gt; list[DocumentChunk]:\n        \"\"\"Performs semantic search over document chunks.\"\"\"\n        pass\n\n# --- Main System Interface ---\nclass MemorySystem:\n    def __init__(self, backend: MemoryBackend):\n        self.backend = backend\n\n    def get_relevant_context(self, last_user_message: str) -&gt; str:\n        \"\"\"The primary method called by the Orchestrator.\"\"\"\n        # Implementation logic described in 5.3\n        pass\n\n    def on_event(self, event: Event):\n        \"\"\"The primary event handler called by the Event Bus Listener.\"\"\"\n        # Implementation logic described in 5.3\n        pass\n</code></pre>"},{"location":"arch/02-state-and-context/#53-key-logic-flows","title":"5.3. Key Logic Flows","text":"<p>This section details the step-by-step logic within the <code>MemorySystem</code>.</p> <p>A. Event Handling (<code>on_event</code> logic):</p> <ol> <li>Receive Event: The <code>on_event</code> method is triggered by the Event Bus Listener.</li> <li>Route by Event Type:<ul> <li>If <code>event.type == \"UserMessageSent\"</code>:</li> <li>Construct an LLM prompt: <code>\"\"\"Does the following user message contain a persistent rule, constraint, or preference for an AI assistant? If so, state the rule clearly in a single imperative sentence. If not, respond with 'N/A'.\\n\\nUser Message: \"{event.text}\"</code>\"\"\"</li> <li>Call the LLM. If the response is not \"N/A\", create a <code>Constraint</code> memory object with the LLM's response as the content and save it via <code>backend.save_memories()</code>.</li> <li>If <code>event.type == \"ToolCallExecuted\"</code> and <code>event.tool_name in [\"run_tests\", \"linter\"]</code> and <code>event.status == \"FAILED\"</code>:</li> <li>Create a <code>HotIssue</code> memory object.</li> <li><code>content</code> should be a summary of the failure (e.g., <code>f\"Tool '{event.tool_name}' failed: {event.result_summary}\"</code>).</li> <li>Save it via <code>backend.save_memories()</code>.</li> <li>If <code>event.type == \"ToolCallExecuted\"</code> and <code>event.status == \"SUCCESS\"</code> and a related <code>HotIssue</code> exists:</li> <li>Retrieve the related <code>HotIssue</code>.</li> <li>Set <code>is_active = False</code> on the <code>HotIssue</code> object.</li> <li>Save the updated memory object.</li> <li>If <code>event.type == \"ToolCallExecuted\"</code> and <code>event.tool_name == \"write_file\"</code>:</li> <li>Read the content of the written file.</li> <li>Chunk the content into manageable pieces (e.g., by paragraph or function definition).</li> <li>Create a <code>DocumentChunk</code> object for each piece.</li> <li>Save them via <code>backend.save_memories()</code>.</li> </ul> </li> </ol> <p>B. Context Retrieval (<code>get_relevant_context</code> logic):</p> <ol> <li>Fetch Active Rules: Call <code>backend.get_active_rules()</code> to get all current <code>Constraint</code> and <code>HotIssue</code> objects.</li> <li>Perform Semantic Search: Call <code>backend.search_documents(query=last_user_message, top_k=5)</code> to get the most relevant document chunks.</li> <li> <p>Format for Prompt: Combine the retrieved memories into a structured string to be injected into the agent's prompt.</p> <pre><code>CONTEXT:\n---\nACTIVE RULES AND ISSUES:\n- [Constraint] Do not use requirements.txt.\n- [Hot Issue] Unit test 'test_payment_flow' is failing.\n\nRELEVANT DOCUMENT SNIPPETS:\n- [Source: src/utils.py] def payment_flow(): ...\n---\n</code></pre> </li> <li> <p>Return the formatted string.</p> </li> </ol>"},{"location":"arch/03-tool-call/","title":"Tool Call and Execution","text":""},{"location":"arch/03-tool-call/#1-overview-and-core-principles","title":"1. Overview and Core Principles","text":"<p>This document details the architecture for defining, executing, and managing tools within AgentX. It is designed to be secure, robust, and extensible. The core principles are:</p> <ul> <li>Security First: Untrusted code (LLM-generated shell commands) must never execute directly on the host machine. All tool execution is centralized and sandboxed.</li> <li>Robust Self-Correction: LLM-generated tool calls can be malformed. The system must be able to detect this, provide corrective feedback to the LLM, and allow it to fix its own mistakes.</li> <li>Structured and Extensible: All tools are strongly-typed and their schemas are automatically generated. The system is designed to easily accommodate new types of tools, including custom user-defined functions and external integrations.</li> </ul>"},{"location":"arch/03-tool-call/#2-tool-definition-and-registration","title":"2. Tool Definition and Registration","text":"<p>A \"tool\" is a capability that an agent can call. This can be a Python function or a shell command.</p>"},{"location":"arch/03-tool-call/#21-tool-definition","title":"2.1. Tool Definition","text":"<ul> <li>Python Functions: Any Python function can be turned into a tool. The function must have type hints for all its arguments and a clear docstring. The docstring is critical as it is used in the prompt to tell the agent what the tool does.</li> </ul> <pre><code>def write_file(path: str, content: str) -&gt; str:\n    \"\"\"Writes content to a file at the specified path.\"\"\"\n    # ... implementation ...\n    return f\"File '{path}' written successfully.\"\n</code></pre> <ul> <li>Shell Commands: Shell commands are defined with a name, description, and an argument schema. The agent's LLM will generate the command string based on the arguments.</li> </ul>"},{"location":"arch/03-tool-call/#22-tool-registration-and-schema-generation","title":"2.2. Tool Registration and Schema Generation","text":"<p>Tools are made available to agents via a central <code>ToolRegistry</code>.</p> <ul> <li>Decorator-Based Registration: The <code>@register_tool</code> decorator is the primary mechanism for adding a tool to the registry.</li> </ul> <pre><code>from agentx.core.tool import register_tool\n\n@register_tool\ndef write_file(path: str, content: str) -&gt; str:\n    # ...\n</code></pre> <ul> <li>Automatic Schema Generation: When a function is decorated, the <code>ToolRegistry</code> automatically inspects its signature and docstring to create a JSON schema. This schema is what the LLM sees and uses to construct a valid tool call. For the <code>write_file</code> example, the generated schema would look like this:</li> </ul> <pre><code>{\n  \"name\": \"write_file\",\n  \"description\": \"Writes content to a file at the specified path.\",\n  \"parameters\": {\n    \"type\": \"object\",\n    \"properties\": {\n      \"path\": { \"type\": \"string\" },\n      \"content\": { \"type\": \"string\" }\n    },\n    \"required\": [\"path\", \"content\"]\n  }\n}\n</code></pre>"},{"location":"arch/03-tool-call/#3-the-tool-call-lifecycle","title":"3. The Tool Call Lifecycle","text":"<p>The following diagram and steps describe the end-to-end flow of a tool call, from generation to result.</p> <pre><code>graph TD\n    subgraph \"Agent\"\n        A[Brain] -- \"1 Generates Tool Call\" --&gt; B(Agent Core)\n    end\n\n    subgraph \"Orchestration Layer\"\n        C[Orchestrator]\n        D[ToolRegistry]\n        E[ToolExecutor]\n    end\n\n    subgraph \"Execution Environment\"\n        F[Python Function]\n        G[Sandbox] -- \"Executes Command\" --&gt; H(Shell Command)\n    end\n\n    B -- \"2 Delegates Call\" --&gt; C\n    C -- \"3 Validates against Schema\" --&gt; D\n    C -- \"4a Validation FAILED\" --&gt; B\n    B -- \"4b Sends Error to LLM for Correction\" --&gt; A\n\n    C -- \"5 Validation PASSED&lt;br/&gt;Dispatches for Execution\" --&gt; E\n\n    E -- \"6a Python Tool\" --&gt; F\n    E -- \"6b Shell Tool\" --&gt; G\n\n    F -- \"7a Result\" --&gt; E\n    H -- \"7b stdout/stderr\" --&gt; G\n    G -- \"7c Result\" --&gt; E\n\n    E -- \"8 Returns Structured Result\" --&gt; C\n    C -- \"9 Forwards Result\" --&gt; B\n    B -- \"10 Formats for LLM &amp;&lt;br/&gt;Adds to Context\" --&gt; A</code></pre> <p>Step-by-Step Flow:</p> <ol> <li> <p>Generation (Agent Brain): The <code>Orchestrator</code> provides the <code>Brain</code> (LLM) with the conversation history and a list of available tool schemas. The LLM determines a tool call is needed and generates the call in a structured format (e.g., JSON).</p> </li> <li> <p>Delegation (Agent Core): The <code>Agent</code>'s core logic receives the raw text from the LLM, parses out the tool call request, and delegates it to the <code>Orchestrator</code> for execution. The agent itself has no permission to execute tools.</p> </li> <li> <p>Validation (Orchestrator): The <code>Orchestrator</code> receives the tool call. It retrieves the corresponding schema from the <code>ToolRegistry</code> and validates the call. It checks for missing required arguments, incorrect types, etc.</p> </li> <li> <p>Self-Correction Loop (If Validation Fails):</p> <ul> <li>If validation fails, the <code>Orchestrator</code> does not attempt to run the tool.</li> <li>It generates a structured error message (e.g., \"Validation Error: Missing required argument 'path' for tool 'write_file'\").</li> <li>This error is sent back to the <code>Agent</code>, which in turn passes it back to the <code>Brain</code> (LLM) in the next turn.</li> <li>The LLM now has the context of its previous failed attempt and the specific validation error, allowing it to generate a corrected tool call.</li> </ul> </li> <li> <p>Dispatch (If Validation Succeeds): If the call is valid, the <code>Orchestrator</code> dispatches it to the <code>ToolExecutor</code>.</p> </li> <li> <p>Secure Execution (ToolExecutor): The <code>ToolExecutor</code> inspects the tool type:</p> <ul> <li>Python Function: The call is a standard Python function call. It is executed directly in the main process.</li> <li>Shell Command: The command is executed inside a secure, isolated sandbox.</li> </ul> </li> <li> <p>Result Capturing: The <code>ToolExecutor</code> captures the result:</p> <ul> <li>For Python functions, this is the return value.</li> <li>For shell commands, this includes <code>stdout</code>, <code>stderr</code>, and the exit code.</li> </ul> </li> <li> <p>Structured Result: The result is packaged into a <code>ToolResult</code> object and returned to the <code>Orchestrator</code>.</p> </li> <li> <p>Forwarding: The <code>Orchestrator</code> forwards the <code>ToolResult</code> to the originating <code>Agent</code>.</p> </li> <li> <p>Contextualization: The <code>Agent</code> formats the result into a clear, readable format (e.g., <code>&lt;tool_result tool_name=\"write_file\"&gt;File 'test.txt' written successfully.&lt;/tool_result&gt;</code>) and adds it to the conversation history for the <code>Brain</code>'s next turn. The LLM can then use this result to form its final response to the user.</p> </li> </ol>"},{"location":"arch/03-tool-call/#4-security-architecture-the-sandbox","title":"4. Security Architecture: The Sandbox","text":"<p>Executing arbitrary, LLM-generated shell commands is a major security risk. AgentX mitigates this by using a sandboxed execution environment.</p> <ul> <li>Technology: The default implementation uses Docker containers. Each shell tool execution spins up a new, short-lived container.</li> <li>Isolation:</li> <li>Network: Containers run with networking disabled by default (<code>--net=none</code>) to prevent exfiltration of data or attacks on the local network. Tools that explicitly need network access must be granted it.</li> <li>Filesystem: The container is only granted access to the current task's workspace directory (<code>/app/workspace</code>). It cannot read from or write to any other part of the host filesystem.</li> <li>Permissions: The process inside the container runs as a non-root user to limit its privileges even within the sandbox.</li> </ul>"},{"location":"arch/03-tool-call/#5-extensibility","title":"5. Extensibility","text":""},{"location":"arch/03-tool-call/#51-custom-and-built-in-tools","title":"5.1. Custom and Built-in Tools","text":"<ul> <li>Built-in Tools: AgentX provides a core set of safe tools for file I/O, search, etc., located in <code>src/agentx/builtin_tools/</code>.</li> <li>Custom Tools: Users can easily define their own tools in their project's codebase. As long as the file is imported and the functions are decorated with <code>@register_tool</code>, they will be available to the agents.</li> </ul>"},{"location":"arch/03-tool-call/#52-mcp-multi-agent-communication-protocol-integration","title":"5.2. MCP (Multi-Agent Communication Protocol) Integration","text":"<p>The tool architecture is extensible to other protocols. An MCP tool could be implemented as a special Python function:</p> <pre><code>@register_tool\ndef send_mcp_message(recipient_agent: str, message_body: str):\n    \"\"\"Sends a message to another agent using the MCP protocol.\"\"\"\n    # Logic to connect to MCP broker and send message\n    # ...\n</code></pre> <p>To the <code>Orchestrator</code> and <code>Agent</code>, this is just another tool. The implementation details are abstracted away.</p>"},{"location":"arch/04-communication/","title":"Communication and Message Architecture","text":""},{"location":"arch/04-communication/#1-core-principles","title":"1. Core Principles","text":"<p>The communication architecture in AgentX is designed to be robust, extensible, and streamable, drawing inspiration from modern standards like the Vercel AI SDK. The core principles are:</p> <ul> <li>Message-Oriented: The fundamental unit of communication is a <code>Message</code>. All interactions, whether from a user, an agent, or a tool, are encapsulated in this structure.</li> <li>Composable Parts: A <code>Message</code> is composed of a list of <code>Part</code> objects. This allows for rich, multi-modal content and a clear separation of concerns (e.g., text, tool calls, and tool results all have their own part type).</li> <li>Streaming First: The entire model is designed for real-time streaming. A <code>Message</code> and its <code>Part</code>s can be sent incrementally, allowing for responsive user interfaces and efficient data flow.</li> </ul>"},{"location":"arch/04-communication/#2-the-message-and-part-schema","title":"2. The <code>Message</code> and <code>Part</code> Schema","text":"<p>This section defines the core data structures for communication.</p>"},{"location":"arch/04-communication/#21-the-message-object","title":"2.1. The <code>Message</code> Object","text":"<p>A <code>Message</code> represents a single entry in the conversation history. It contains a list of <code>Part</code> objects that make up its content.</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Union, Literal, Dict, Any\nimport uuid\n\nclass Message(BaseModel):\n    \"\"\"\n    Represents a single message in the conversation, composed of multiple parts.\n    \"\"\"\n    id: str = Field(default_factory=lambda: f\"msg_{uuid.uuid4().hex}\")\n    role: Literal[\"user\", \"assistant\"]\n    parts: List[Union[\n        \"TextPart\",\n        \"ToolCallPart\",\n        \"ToolResultPart\",\n        \"ToolErrorPart\"\n    ]]\n</code></pre>"},{"location":"arch/04-communication/#22-the-part-objects","title":"2.2. The <code>Part</code> Objects","text":"<p>These are the building blocks of a <code>Message</code>.</p>"},{"location":"arch/04-communication/#textpart","title":"<code>TextPart</code>","text":"<p>The simplest part, containing a piece of text. During streaming, multiple <code>TextPart</code> objects can be sent to represent a continuous flow of text.</p> <pre><code>class TextPart(BaseModel):\n    type: Literal[\"text\"] = \"text\"\n    text: str\n</code></pre>"},{"location":"arch/04-communication/#toolcallpart","title":"<code>ToolCallPart</code>","text":"<p>A structured request from an agent to call a specific tool with a given set of arguments.</p> <pre><code>class ToolCallPart(BaseModel):\n    type: Literal[\"tool_call\"] = \"tool_call\"\n    tool_call_id: str = Field(default_factory=lambda: f\"tc_{uuid.uuid4().hex}\")\n    tool_name: str\n    args: Dict[str, Any]\n</code></pre>"},{"location":"arch/04-communication/#toolresultpart","title":"<code>ToolResultPart</code>","text":"<p>The successful result of a tool execution. It is explicitly linked back to the originating call by its <code>tool_call_id</code>.</p> <pre><code>class ToolResultPart(BaseModel):\n    type: Literal[\"tool_result\"] = \"tool_result\"\n    tool_call_id: str\n    result: Any  # The structured output from the tool\n</code></pre>"},{"location":"arch/04-communication/#toolerrorpart","title":"<code>ToolErrorPart</code>","text":"<p>Used when a tool call fails, either due to a validation error or an execution error. This is the key to the self-correction loop.</p> <pre><code>class ToolErrorPart(BaseModel):\n    type: Literal[\"tool_error\"] = \"tool_error\"\n    tool_call_id: str\n    error_type: Literal[\"VALIDATION\", \"EXECUTION\"]\n    message: str # A human-readable error message for the LLM\n</code></pre>"},{"location":"arch/04-communication/#3-example-message-flows","title":"3. Example Message Flows","text":""},{"location":"arch/04-communication/#31-simple-text-conversation","title":"3.1. Simple Text Conversation","text":"<p>User Message:</p> <pre><code>{\n  \"role\": \"user\",\n  \"parts\": [{ \"type\": \"text\", \"text\": \"Hello, world!\" }]\n}\n</code></pre> <p>Assistant Response (streamed):</p> <pre><code>// Stream Packet 1\n{ \"role\": \"assistant\", \"parts\": [{ \"type\": \"text\", \"text\": \"Hello\" }] }\n// Stream Packet 2\n{ \"role\": \"assistant\", \"parts\": [{ \"type\": \"text\", \"text\": \" there!\" }] }\n</code></pre>"},{"location":"arch/04-communication/#32-successful-tool-call-flow","title":"3.2. Successful Tool Call Flow","text":"<p>This example shows the full lifecycle, from the agent requesting a tool to receiving the result.</p> <p>1. Assistant requests a tool call: The agent's message contains both explanatory text and a <code>ToolCallPart</code>.</p> <pre><code>{\n  \"role\": \"assistant\",\n  \"parts\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"Sure, I can write that file for you. I will now call the tool.\"\n    },\n    {\n      \"type\": \"tool_call\",\n      \"tool_call_id\": \"tc_123\",\n      \"tool_name\": \"write_file\",\n      \"args\": { \"path\": \"/hello.txt\", \"content\": \"Hello, world!\" }\n    }\n  ]\n}\n</code></pre> <p>2. The system provides the tool result: A new message is created containing the <code>ToolResultPart</code>. This is sent back to the agent for its next reasoning step.</p> <pre><code>{\n  \"role\": \"assistant\",\n  \"parts\": [\n    {\n      \"type\": \"tool_result\",\n      \"tool_call_id\": \"tc_123\",\n      \"result\": { \"status\": \"success\", \"message\": \"File written successfully.\" }\n    }\n  ]\n}\n</code></pre> <p>3. The agent generates its final response: After receiving the result, the agent formulates its final answer.</p> <pre><code>{\n  \"role\": \"assistant\",\n  \"parts\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"I have successfully written the file to `/hello.txt`.\"\n    }\n  ]\n}\n</code></pre>"},{"location":"arch/04-communication/#33-tool-call-with-self-correction-validation-error","title":"3.3. Tool Call with Self-Correction (Validation Error)","text":"<p>This flow demonstrates how <code>ToolErrorPart</code> enables the agent to fix its own mistakes.</p> <p>1. Assistant makes a malformed tool call (missing <code>path</code>):</p> <pre><code>{\n  \"role\": \"assistant\",\n  \"parts\": [\n    {\n      \"type\": \"tool_call\",\n      \"tool_call_id\": \"tc_456\",\n      \"tool_name\": \"write_file\",\n      \"args\": { \"content\": \"This call is missing the path.\" }\n    }\n  ]\n}\n</code></pre> <p>2. The <code>Orchestrator</code> returns a <code>ToolErrorPart</code>: Instead of executing the tool, the system provides a structured validation error.</p> <pre><code>{\n  \"role\": \"assistant\",\n  \"parts\": [\n    {\n      \"type\": \"tool_error\",\n      \"tool_call_id\": \"tc_456\",\n      \"error_type\": \"VALIDATION\",\n      \"message\": \"Validation failed for tool 'write_file': Missing required argument 'path'.\"\n    }\n  ]\n}\n</code></pre> <p>3. The agent receives the error and provides a corrected call: The error message is now part of the agent's context. Its <code>Brain</code> sees the mistake and generates a valid call in the next turn.</p> <pre><code>{\n  \"role\": \"assistant\",\n  \"parts\": [\n    {\n      \"type\": \"text\",\n      \"text\": \"My apologies, I missed a required parameter. I will correct it now.\"\n    },\n    {\n      \"type\": \"tool_call\",\n      \"tool_call_id\": \"tc_789\",\n      \"tool_name\": \"write_file\",\n      \"args\": {\n        \"path\": \"/corrected.txt\",\n        \"content\": \"This call should now be valid.\"\n      }\n    }\n  ]\n}\n</code></pre> <p>This new message now proceeds down the successful tool call path.</p>"},{"location":"arch/04-communication/#4-streaming-and-progressive-responses","title":"4. Streaming and Progressive Responses","text":"<p>To build a truly responsive UI, a client needs to be aware of the agent's progress throughout its turn, not just at the end. This is especially important for multi-step operations like tool calls. AgentX achieves this by streaming a sequence of <code>Message</code> updates for a single logical turn.</p>"},{"location":"arch/04-communication/#41-the-streaming-protocol","title":"4.1. The Streaming Protocol","text":"<p>The client should not assume a single request will yield a single response. Instead, it should be prepared to receive multiple, distinct <code>Message</code> objects over the same streaming connection. Each object provides a real-time snapshot of the agent's activity.</p> <p>The following sequence diagram illustrates how a client receives progressive updates during a tool call.</p> <pre><code>sequenceDiagram\n    participant Client\n    participant AgentX\n    participant Agent\n    participant ToolExecutor\n\n    Client-&gt;&gt;AgentX: POST /chat (User Message)\n\n    activate AgentX\n    AgentX-&gt;&gt;Agent: invoke\n\n    Agent--&gt;&gt;AgentX: Generate ToolCallPart\n    AgentX--&gt;&gt;Client: Stream Packet 1 (Message with ToolCallPart)\n    Note over Client: UI displays \"Calling write_file...\"\n\n    AgentX-&gt;&gt;ToolExecutor: execute(tool_call)\n    ToolExecutor--&gt;&gt;AgentX: result\n\n    AgentX--&gt;&gt;Client: Stream Packet 2 (Message with ToolResultPart)\n    Note over Client: UI displays \"Tool successful.\"\n\n    AgentX-&gt;&gt;Agent: provide_result\n    Agent-&gt;&gt;AgentX: Generate TextPart stream\n\n    loop Text Streaming\n        AgentX--&gt;&gt;Client: Stream Packet 3...N (Message with TextPart chunks)\n    end\n    Note over Client: UI streams final text response.\n\n    deactivate AgentX</code></pre>"},{"location":"arch/04-communication/#42-example-streamed-packets","title":"4.2. Example Streamed Packets","text":"<p>Here is what the client would see for the successful tool call flow from section 3.2, but as a real-time stream.</p> <p>1. Client sends a request: <code>POST /chat</code> with a user message asking to write a file.</p> <p>2. AgentX streams back the agent's decision to call the tool: The UI can immediately show that a tool is being used.</p> <pre><code>{\n  \"role\": \"assistant\",\n  \"id\": \"msg_abc\",\n  \"parts\": [\n    {\n      \"type\": \"tool_call\",\n      \"tool_call_id\": \"tc_123\",\n      \"tool_name\": \"write_file\",\n      \"args\": { \"path\": \"/hello.txt\", \"content\": \"Hello, world!\" }\n    }\n  ]\n}\n</code></pre> <p>3. AgentX streams the result of the tool execution: The UI knows the tool call has completed successfully before the agent has formulated its final textual response.</p> <pre><code>{\n  \"role\": \"assistant\",\n  \"id\": \"msg_abc\",\n  \"parts\": [\n    {\n      \"type\": \"tool_result\",\n      \"tool_call_id\": \"tc_123\",\n      \"result\": { \"status\": \"success\", \"message\": \"File written successfully.\" }\n    }\n  ]\n}\n</code></pre> <p>4. AgentX streams the final text response, token by token: The <code>id</code> field links these text chunks back to the same logical message.</p> <pre><code>// Packet 1\n{ \"role\": \"assistant\", \"id\": \"msg_abc\", \"parts\": [{ \"type\": \"text\", \"text\": \"I have\" }] }\n// Packet 2\n{ \"role\": \"assistant\", \"id\": \"msg_abc\", \"parts\": [{ \"type\": \"text\", \"text\": \" successfully\" }] }\n// Packet 3\n{ \"role\": \"assistant\", \"id\": \"msg_abc\", \"parts\": [{ \"type\": \"text\", \"text\": \" written the file.\" }] }\n</code></pre> <p>This progressive, part-based streaming model gives a UI all the information it needs to render a rich, real-time view of the agent's work.</p>"}]}